{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUdjG4-XsE0I"
      },
      "source": [
        "# **ColabSaProt: Collaborative Protein Language Modeling**\n",
        "\n",
        "This is the Colab version of [SaProt](https://github.com/westlake-repl/SaProt), a pre-trained protein language model designed for various downstream protein tasks.\n",
        "\n",
        "**ColabSaProt** is a platform where **Protein Language Models(PLMs)** are more accessible and user-friendly for biologists, enabling effortless model training and sharing within the scientific community.\n",
        "\n",
        "We've established the [SaProtHub](https://huggingface.co/SaProtHub) for storing and sharing models and datasets, where you can explore extensive collections for specific protein prediction tasks.\n",
        "\n",
        "We hope ColabSaProt and SaProtHub can contribute to advancing biological research, fostering collaboration, and accelerating discoveries in the field. You can access [our paper](https://www.biorxiv.org/content/10.1101/2023.10.01.560349v2) for further details.\n",
        "\n",
        "For detailed steps of each section, please refer to the <a href=\"#manual\">manual</a>.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nLb_im9sJWw"
      },
      "source": [
        "## ColabSaProt Content\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/content.png\n",
        "?raw=true\" height=\"400\" width=\"800px\" align=\"center\">\n",
        "\n",
        "<font color=red>**To view the content, please click on the first option in the left sidebar.**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVQ6vaQTjYO3"
      },
      "source": [
        "## SaProt Hub\n",
        "\n",
        "Find awesome models and datasets for specific protein task on [SaProtHub](https://huggingface.co/SaProtHub)!\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/SaProtHub.png?raw=true\" height=\"500\" width=\"800px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnTxW38rBI7d"
      },
      "source": [
        "# 0: Instruction\n",
        "\n",
        "Before you begin training and utilizing your model, here are some important details about **Task**, **Dataset** and **Basic Colab knowledge** that you need to be aware of.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bygnx_yiBI7d"
      },
      "source": [
        "## 0.1: Task\n",
        "\n",
        "Different models are designed for different tasks, so it's essential to understand **which type your task belongs to**.\n",
        "\n",
        "You can recognize your task type based on your task description and objectives.\n",
        "\n",
        "<br>\n",
        "\n",
        "<!-- ### Task Type\n",
        "\n",
        "- **Classification Task**: classify protein sequences.\n",
        "- **Regression Task**: predict the value of some property of a protein sequence.\n",
        "- **Amino Acid Classification Task**: classify the amino acids in a protein sequence.  -->\n",
        "\n",
        "\n",
        "| Task Type                             | Description                                                  |\n",
        "| ------------------------------------- | ------------------------------------------------------------ |\n",
        "| **Classification Task**               | Classify protein sequences.                                  |\n",
        "| **Regression Task**                   | Predict the value of some property of a protein sequence.    |\n",
        "| **Amino Acid Classification Task**    | Classify the amino acids in a protein sequence.              |\n",
        "| **Mutational Effect Prediction Task** | Predict the mutational effect based on the wild type sequence and mutation information. |\n",
        "| **Inverse Folding Prediction**        | Predict the residue sequence of a structure-aware sequence with masked amino acids (which could be all masked or partially masked). |\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Here are some example tasks and their task type:\n",
        "\n",
        "| Task Type | Example |\n",
        "| --- | --- |\n",
        "| **Classification Task** | **Subcellular Location Prediction**: predict which location category the protein belong to. |\n",
        "| **Classification Task** | **Metal Ion Binding Detection**: predict whether there are metal ion–binding sites in the protein. |\n",
        "| **Regression Task** | **Thermostability Prediction**: predict the thermostability value of a protein. |\n",
        "| **Amino Acid Classification Task** | **Binding Site Detection**: predict whether the amino acid is a binding site or not. |\n",
        "\n",
        "<!-- <br>\n",
        "\n",
        "### Use your models or shared models on SaProtHub\n",
        "\n",
        "You can use\n",
        "\n",
        "- your trained model\n",
        "- or shared models on SaProtHub\n",
        "- pre-trained protein language model\n",
        "\n",
        "to make some prediction -->\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acS6TsJCBI7d"
      },
      "source": [
        "## 0.2: Dataset <a name=\"data_format\"></a>\n",
        "\n",
        "You can use your private data to train and predict. Below are the various data formats corresponding to different **data types**.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### Data Type\n",
        "\n",
        "1. Single AA Sequence\n",
        "2. Single SA Sequence\n",
        "3. Single UniProt ID\n",
        "4. Single PDB/CIF Structure\n",
        "5. Multiple AA Sequences\n",
        "6. Multiple SA Sequences\n",
        "7. Multiple UniProt IDs\n",
        "8. Multiple PDB/CIF Structures\n",
        "9. Huggingface Dataset\n",
        "\n",
        "<br>\n",
        "\n",
        "### Data Format <a name='data_format'></a>\n",
        "\n",
        "#### For `Single AA Sequence`, `Single SA Sequence`, and `Single UniProt ID` (first three data types)\n",
        "An input box will appear after running the cell. Please enter the protein sequence in the required format.\n",
        "\n",
        "<br>\n",
        "\n",
        "####  For `Single PDB/CIF Structure` (fourth data type)\n",
        "A file upload button will appear after running the cell. Please upload a .pdb or .cif file.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### For `Multiple AA Sequences`, `Multiple SA Sequences`, `Multiple UniProt IDs` (fifth to seventh data types)\n",
        "A file upload button will appear after running the cell. Please upload a .csv file and ensure that the column name in the .csv file is `Sequence`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_Sequences_data_format.png?raw=true\" height=\"200\" width=\"800px\" align=\"center\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "#### For `Multiple PDB/CIF Structures`\n",
        "A file upload button will appear after running the cell. Please upload a .csv file containing three columns: `Sqeuence`, `type` and `chain`;\n",
        "\n",
        "- `type`: Indicate whether the structure file is a real PDB structure or an AlphaFold 2 predicted structure. For AF2 (AlphaFold 2) structures, we will apply pLDDT masking. The value must be either \"PDB\" or \"AF2\".\n",
        "- `chain`: For real PDB structures, since multiple chains may exist in one .pdb file, it is necessary to specify which chain is used. For AF2 structures, the chain is assumed to be A by default.\n",
        "\n",
        "After successfully uploading the .csv file, a second file upload button will appear. Please upload a zip file containing all corresponding pdb/cif files.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_PDB_CIF_Structures_data_format.png?raw=true\" height=\"200\" width=\"500px\" align=\"center\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "#### For `Huggingface Dataset`\n",
        "An input box will appear after running the cell. Please enter the the repo_id of the Huggingface Dataset. Find some datasets in [Official SaProtHub Repository](https://huggingface.co/SaProtHub).\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Huggingface_ID.png?raw=true\" height=\"200\" width=\"600px\" align=\"center\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### SA(Structure-aware) Sequence\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/SA_Sequence.png?raw=true\" height=\"300\" width=\"600px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Y0sEorAF4a"
      },
      "source": [
        "## 0.3: Colab\n",
        "\n",
        "<br>\n",
        "\n",
        "### Cell Running status\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Cell_status.png?raw=true\" height=\"400\" width=\"600px\" align=\"center\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dw1U1uBI7d"
      },
      "source": [
        "# **1: Installation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RQmyJm-3Qoii"
      },
      "outputs": [],
      "source": [
        "#@title 1.1: ⚠️ Switch your Runtime type to <font color=red>**GPU!!!**</font>\n",
        "\n",
        "#@markdown You can check the current runtime type in <font color=red>**the upper right corner of the page**</font>. If the current runtime type is CPU, you need to <font color=red>**switch it to GPU (either the free T4 or the paid A100)**</font> for a better training experience.\n",
        "\n",
        "#@markdown <img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Runtime.png?raw=true\" height=\"300\" width=\"400px\" align=\"center\">\n",
        "\n",
        "#@markdown #### Please follow the steps below to switch the runtime to GPU:\n",
        "\n",
        "#@markdown 1. Click the dropdown button\n",
        "#@markdown 2. Select option \"Change runtime type\"\n",
        "#@markdown 3. Select a GPU\n",
        "#@markdown 4. Click \"Save\" button\n",
        "#@markdown 5. <font color=red>Each time you switch the runtime, all code blocks need to be **re-executed**.</font>\n",
        "\n",
        "\n",
        "#@markdown <img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Switch_Runtime.png?raw=true\" height=\"400\" width=\"800px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgvb8ibwBI7d",
        "outputId": "7132f922-1e98-4496-9a01-6c4f1c0d3ca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installation finished!\n"
          ]
        }
      ],
      "source": [
        "#@title 1.2: Click the run button ▶️ to install SaProt\n",
        "\n",
        "#@markdown (Please waiting for 2-8 minutes to install...)\n",
        "################################################################################\n",
        "########################### install saprot #####################################\n",
        "################################################################################\n",
        "\n",
        "try:\n",
        "  import saprot\n",
        "  print(\"SaProt is installed successfully!\")\n",
        "except ImportError:\n",
        "  print(\"Installing SaProt...\")\n",
        "  !mkdir -p /content/saprot/LMDB\n",
        "  !mkdir -p /content/saprot/bin\n",
        "  !mkdir -p /content/saprot/output\n",
        "  !mkdir -p /content/saprot/adapters/classification\n",
        "  !mkdir -p /content/saprot/adapters/regression\n",
        "  !mkdir -p /content/saprot/adapters/token_classification\n",
        "  !mkdir -p /content/saprot/structures\n",
        "\n",
        "  !pip install colorama --quiet\n",
        "  !pip install gdown==v4.6.3 --force-reinstall --quiet\n",
        "  !gdown https://drive.google.com/drive/folders/1ECKe5clJXs4POlScVggRQDrFo5HJpGBN?usp=drive_link -O /content/saprot/ --folder  --quiet && pip install /content/saprot/ColabSaProtSetup/saprot-0.4.5-py3-none-any.whl --quiet\n",
        "  !chmod +x /content/saprot/ColabSaProtSetup/foldseek\n",
        "\n",
        "  !rsync -a --remove-source-files /content/saprot/ColabSaProtSetup/upload_files /content/saprot\n",
        "  !rsync -a --remove-source-files /content/saprot/ColabSaProtSetup/datasets /content/saprot\n",
        "  !mv /content/saprot/ColabSaProtSetup/foldseek /content/saprot/bin/\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################## global ######################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "import ipywidgets\n",
        "from google.colab import widgets\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "from loguru import logger\n",
        "\n",
        "import yaml\n",
        "import argparse\n",
        "\n",
        "from easydict import EasyDict\n",
        "\n",
        "from colorama import init, Fore, Back, Style\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import subprocess\n",
        "\n",
        "from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "from huggingface_hub import snapshot_download\n",
        "import json\n",
        "\n",
        "DATASET_HOME = Path('/content/saprot/datasets')\n",
        "ADAPTER_HOME = Path('/content/saprot/adapters')\n",
        "STRUCTURE_HOME = Path(\"/content/saprot/structures\")\n",
        "LMDB_HOME = Path('/content/saprot/LMDB')\n",
        "OUTPUT_HOME = Path('/content/saprot/output')\n",
        "UPLOAD_FILE_HOME = Path('/content/saprot/upload_files')\n",
        "FOLDSEEK_PATH = Path(\"/content/saprot/bin/foldseek\")\n",
        "aa_set = {\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"}\n",
        "foldseek_struc_vocab = \"pynwrqhgdlvtmfsaeikc#\"\n",
        "\n",
        "task_type_dict = {\n",
        "  \"Classify protein sequences (classification)\" : \"classification\",\n",
        "  \"Classify each Amino Acid (amino acid classification), e.g. Binding site detection\" : \"token_classification\",\n",
        "  \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\" : \"regression\",\n",
        "}\n",
        "model_type_dict = {\n",
        "  \"classification\" : \"saprot/saprot_classification_model\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_model\",\n",
        "  \"regression\" : \"saprot/saprot_regression_model\",\n",
        "}\n",
        "dataset_type_dict = {\n",
        "  \"classification\": \"saprot/saprot_classification_dataset\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_dataset\",\n",
        "  \"regression\": \"saprot/saprot_regression_dataset\",\n",
        "}\n",
        "class font:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "    RESET = '\\033[0m'\n",
        "\n",
        "def get_adapters_list():\n",
        "\n",
        "    adapters_list = []\n",
        "\n",
        "    for file_path in ADAPTER_HOME.glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.parent)\n",
        "\n",
        "    return adapters_list\n",
        "\n",
        "\n",
        "def show_adapters_info(adapters_list):\n",
        "  grid = widgets.Grid(len(adapters_list)+1, 2, header_row=True, header_column=True)\n",
        "\n",
        "  with grid.output_to(0, 0):\n",
        "    print(\"ID\")\n",
        "\n",
        "  with grid.output_to(0, 1):\n",
        "    print(\"Local Model\")\n",
        "\n",
        "  # with grid.output_to(0, 2):\n",
        "  #   print(\"Adapter Path\")\n",
        "\n",
        "  for i in range(len(adapters_list)):\n",
        "    with grid.output_to(i+1, 0):\n",
        "      print(i)\n",
        "    with grid.output_to(i+1, 1):\n",
        "      print(adapters_list[i].stem)\n",
        "    # with grid.output_to(i+1, 2):\n",
        "    #   print(adapters_list[i])\n",
        "\n",
        "def adapters_text(adapters_list):\n",
        "  input = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Enter Huggingface Model ID',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  input.layout.width = '500px'\n",
        "  display(input)\n",
        "\n",
        "  return input\n",
        "\n",
        "def adapters_dropdown(adapters_list):\n",
        "  dropdown = ipywidgets.Dropdown(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  dropdown.layout.width = '500px'\n",
        "  display(dropdown)\n",
        "\n",
        "  return dropdown\n",
        "\n",
        "def adapters_combobox(adapters_list):\n",
        "  combobox = ipywidgets.Combobox(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Enter Huggingface Model repository id or select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  combobox.layout.width = '500px'\n",
        "  display(combobox)\n",
        "\n",
        "  return combobox\n",
        "\n",
        "def select_adapter():\n",
        "  adapters_list = get_adapters_list()\n",
        "  print(Fore.BLUE+\"Existing Models:\"+Style.RESET_ALL)\n",
        "  # print(\"=\"*100)\n",
        "  # show_adapters_info(adapters_list)\n",
        "  # print(\"=\"*100)\n",
        "  return adapters_combobox(adapters_list)\n",
        "\n",
        "def select_adapter_from(use_model_from):\n",
        "  adapters_list = get_adapters_list()\n",
        "\n",
        "  if use_model_from == 'Local Models':\n",
        "    print(Fore.BLUE+\"Local Model:\"+Style.RESET_ALL)\n",
        "    return adapters_dropdown(adapters_list)\n",
        "  elif use_model_from == 'SaProtHub Models':\n",
        "    print(Fore.BLUE+\"Huggingface Model:\"+Style.RESET_ALL)\n",
        "    return adapters_text(adapters_list)\n",
        "\n",
        "################################################################################\n",
        "########################### download dataset ###################################\n",
        "################################################################################\n",
        "def download_dataset(task_name):\n",
        "  import gdown\n",
        "  import tarfile\n",
        "\n",
        "  filepath = LMDB_HOME / f\"{task_name}.tar.gz\"\n",
        "  download_links = {\n",
        "    \"ClinVar\" : \"https://drive.google.com/uc?id=1Le6-v8ddXa1eLJZFo7HPij7NhaBmNUbo\",\n",
        "    \"DeepLoc_cls2\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"DeepLoc_cls10\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"EC\" : \"https://drive.google.com/uc?id=1VFLFA-jK1tkTZBVbMw8YSsjZqAqlVQVQ\",\n",
        "    \"GO_BP\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_CC\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_MF\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"HumanPPI\" : \"https://drive.google.com/uc?id=1ahgj-IQTtv3Ib5iaiXO_ASh2hskEsvoX\",\n",
        "    \"MetalIonBinding\" : \"https://drive.google.com/uc?id=1rwknPWIHrXKQoiYvgQy4Jd-efspY16x3\",\n",
        "    \"ProteinGym\" : \"https://drive.google.com/uc?id=1L-ODrhfeSjDom-kQ2JNDa2nDEpS8EGfD\",\n",
        "    \"Thermostability\" : \"https://drive.google.com/uc?id=1I9GR1stFDHc8W3FCsiykyrkNprDyUzSz\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    gdown.download(download_links[task_name], str(filepath), quiet=False)\n",
        "    with tarfile.open(filepath, 'r:gz') as tar:\n",
        "      tar.extractall(path=str(LMDB_HOME))\n",
        "      print(f\"Extracted: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"The dataset has not prepared.\")\n",
        "\n",
        "################################################################################\n",
        "############################# upload file ######################################\n",
        "################################################################################\n",
        "def upload_file(upload_path):\n",
        "  import shutil\n",
        "  import os\n",
        "  from pathlib import Path\n",
        "  import sys\n",
        "\n",
        "  upload_path = Path(upload_path)\n",
        "  upload_path.mkdir(parents=True, exist_ok=True)\n",
        "  basepath = Path().resolve()\n",
        "  try:\n",
        "    uploaded = files.upload()\n",
        "    filenames = []\n",
        "    for filename in uploaded.keys():\n",
        "      filenames.append(filename)\n",
        "      shutil.move(basepath / filename, upload_path / filename)\n",
        "    if len(filenames) == 0:\n",
        "      logger.info(\"The uploading process has been interrupted by the user.\")\n",
        "      raise RuntimeError(\"The uploading process has been interrupted by the user.\")\n",
        "  except Exception as e:\n",
        "    logger.error(\"Upload file fail! Please click the button to run again.\")\n",
        "    raise(e)\n",
        "\n",
        "  return upload_path / filenames[0]\n",
        "\n",
        "################################################################################\n",
        "############################ upload dataset ####################################\n",
        "################################################################################\n",
        "\n",
        "data_type_list = [\"Single AA Sequence\",\n",
        "                  \"Single SA Sequence\",\n",
        "                  \"Single UniProt ID\",\n",
        "                  \"Single PDB/CIF Structure\",\n",
        "                  \"Multiple AA Sequences\",\n",
        "                  \"Multiple SA Sequences\",\n",
        "                  \"Multiple UniProt IDs\",\n",
        "                  \"Multiple PDB/CIF Structures\",\n",
        "                  \"Huggingface Dataset\"]\n",
        "\n",
        "def input_raw_data_by_data_type(data_type):\n",
        "  print(Fore.BLUE+\"Dataset: \"+Style.RESET_ALL, end='')\n",
        "\n",
        "  # 0-2. 0. Single AA Sequence, 1. Single SA Sequence, 2. Single UniProt ID\n",
        "  if data_type in data_type_list[:3]:\n",
        "    input_seq = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter {data_type} here',\n",
        "      disabled=False)\n",
        "    input_seq.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_seq)\n",
        "    return input_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  elif data_type == data_type_list[3]:\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "    return pdb_file_path.stem\n",
        "\n",
        "  # 4-7. Multiple Sequences\n",
        "  elif data_type in data_type_list[4:8]:\n",
        "    print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "    uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if data_type == data_type_list[7]:\n",
        "      # upload and unzip PDB files\n",
        "      print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "      pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "      if pdb_zip_path.suffix != \".zip\":\n",
        "        logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "        raise RuntimeError(\"The data type does not match.\")\n",
        "      print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "      print(\"=\"*100)\n",
        "\n",
        "      import zipfile\n",
        "      with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "    return uploaded_csv_path\n",
        "\n",
        "  # 8. Huggingface Dataset\n",
        "  elif data_type == data_type_list[8]:\n",
        "    input_repo_id = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter {data_type} repository id here',\n",
        "      disabled=False)\n",
        "    input_repo_id.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_repo_id)\n",
        "    return input_repo_id\n",
        "\n",
        "\n",
        "def get_SA_sequence_by_data_type(data_type, raw_data):\n",
        "\n",
        "  # 0. Single AA Sequence\n",
        "  if data_type == data_type_list[0]:\n",
        "    input_seq = raw_data\n",
        "    aa_seq = input_seq.value\n",
        "\n",
        "    sa_seq = ''\n",
        "    for aa in aa_seq:\n",
        "        sa_seq += aa + '#'\n",
        "    return sa_seq\n",
        "\n",
        "  # 1. Single SA Sequence\n",
        "  if data_type == data_type_list[1]:\n",
        "    input_seq = raw_data\n",
        "    sa_seq = input_seq.value\n",
        "\n",
        "    return sa_seq\n",
        "\n",
        "  # 2. Single UniProt ID\n",
        "  if data_type == data_type_list[2]:\n",
        "    input_seq = raw_data\n",
        "    uniprot_id = input_seq.value\n",
        "\n",
        "    protein_list = [uniprot_id]\n",
        "    uniprot2pdb(protein_list)\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=1, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "    sa_seq = seqs[0].split('\\t')[1]\n",
        "    return sa_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  if data_type == data_type_list[3]:\n",
        "    uniprot_id = raw_data\n",
        "\n",
        "    protein_list = [uniprot_id]\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=1, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "    sa_seq = seqs[0].split('\\t')[1]\n",
        "    return sa_seq\n",
        "\n",
        "  # raw_data = upload_files/xxx.csv\n",
        "  if data_type in data_type_list[4:8]:\n",
        "    uploaded_csv_path = raw_data\n",
        "    csv_dataset_path = DATASET_HOME / uploaded_csv_path.name\n",
        "\n",
        "  # 4. Multiple AA Sequences\n",
        "  if data_type == data_type_list[4]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    for index, value in protein_df['Sequence'].items():\n",
        "      sa_seq = ''\n",
        "      for aa in value:\n",
        "        sa_seq += aa + '#'\n",
        "      protein_df.at[index, 'Sequence'] = sa_seq\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 5. Multiple SA Sequences\n",
        "  if data_type == data_type_list[5]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 6. Multiple UniProt IDs\n",
        "  if data_type == data_type_list[6]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    protein_list = protein_df.iloc[:, 0].tolist()\n",
        "    uniprot2pdb(protein_list)\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=1, return_results=True)\n",
        "    outputs = mprs.run()\n",
        "\n",
        "    protein_df['Sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 7. Multiple PDB/CIF Structures\n",
        "  if data_type == data_type_list[7]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    # protein_list = [(uniprot_id, type, chain), ...]\n",
        "    # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "    # uniprot2pdb(protein_list)\n",
        "    protein_list = []\n",
        "    for tuple_row in df.itertuples(index=False):\n",
        "      assert tuple_row.type in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "      protein_list.append(tuple_row)\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=1, return_results=True)\n",
        "    outputs = mprs.run()\n",
        "\n",
        "    protein_df['Sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # # 7. Multiple PDB/CIF Structures (AF2)\n",
        "  # if data_type == data_type_list[7]:\n",
        "  #   protein_df = pd.read_csv(uploaded_csv_path)\n",
        "  #   protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "  #   # uniprot2pdb(protein_list)\n",
        "  #   mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=1, return_results=True)\n",
        "  #   outputs = mprs.run()\n",
        "\n",
        "  #   protein_df['Sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "  #   protein_df.to_csv(csv_dataset_path, index=None)\n",
        "  #   return csv_dataset_path\n",
        "\n",
        "  # 8. Huggingface Dataset\n",
        "  if data_type == data_type_list[8]:\n",
        "    input_repo_id = raw_data\n",
        "    REPO_ID = input_repo_id.value\n",
        "\n",
        "    if REPO_ID.startswith('/'):\n",
        "      return Path(REPO_ID)\n",
        "\n",
        "    snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\", local_dir=LMDB_HOME/REPO_ID)\n",
        "\n",
        "    return LMDB_HOME/REPO_ID\n",
        "\n",
        "\n",
        "# # return a SA Sequence or a csv dataset path\n",
        "# def get_raw_dataset(data_type, raw_data):\n",
        "#   if data_type in data_type_list[:3]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data.value)\n",
        "#   elif data_type == data_type_list[3]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "#   elif data_type in data_type_list[4:8]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "#   elif data_type in data_type_list[8]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data.value)\n",
        "\n",
        "#   return raw_dataset\n",
        "\n",
        "# def upload_dataset(data_type):\n",
        "#   print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "#   uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#   print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "#   print(\"=\"*100)\n",
        "\n",
        "#   # selected_csv_dataset = DATASET_HOME / f\"[DATASET]{Path(uploaded_csv_path).stem}.csv\"\n",
        "#   # get_SASequence_by_data_type(data_type, uploaded_csv_path, selected_csv_dataset)\n",
        "#   # get_SA_sequence_by_data_type(data_type, uploaded_csv_path)\n",
        "#   # print()\n",
        "#   # print(\"=\"*100)\n",
        "#   # print(Fore.BLUE+\"Successfully upload your dataset!\"+Style.RESET_ALL)\n",
        "\n",
        "#   return uploaded_csv_path\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################## Download predicted structures #######################\n",
        "################################################################################\n",
        "def uniprot2pdb(uniprot_ids, nprocess=20):\n",
        "  from saprot.utils.downloader import AlphaDBDownloader\n",
        "\n",
        "  os.makedirs(STRUCTURE_HOME, exist_ok=True)\n",
        "  af2_downloader = AlphaDBDownloader(uniprot_ids, \"pdb\", save_dir=STRUCTURE_HOME, n_process=20)\n",
        "  af2_downloader.run()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############### Form foldseek sequences by multiple processes ##################\n",
        "################################################################################\n",
        "# def pdb2sequence(process_id, idx, uniprot_id, writer):\n",
        "#   from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "#   try:\n",
        "#     pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "#     cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "#     if Path(pdb_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "#     if Path(cif_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "\n",
        "#     writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "# clear_output(wait=True)\n",
        "# print(\"Installation finished!\")\n",
        "\n",
        "def pdb2sequence(process_id, idx, row_tuple, writer):\n",
        "\n",
        "  uniprot_id = row_tuple.Sequence.split('.')[0] #\n",
        "  struc_type = row_tuple.type                   # PDB or AF2\n",
        "  chain = row_tuple.chain                       #\n",
        "\n",
        "\n",
        "  from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "  try:\n",
        "    pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "    cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "    if Path(pdb_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [chain], process_id=process_id)[chain][-1]\n",
        "    if Path(cif_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [chain], process_id=process_id)[chain][-1]\n",
        "\n",
        "    writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Installation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uxag_RSBI7e"
      },
      "source": [
        "# **2: Train and Share your Protein Model**\n",
        "\n",
        "## Training Dataset\n",
        "\n",
        "For the training dataset, **two additional columns** are required in the CSV file: `label` and `stage`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_AA_Sequences_data_format_training.png\n",
        "?raw=true\" height=\"200\" width=\"400px\" align=\"center\">\n",
        "\n",
        "- `label`: The content of this column depends on your **task type**:\n",
        "\n",
        "| Task Type                         | Content in the Column                          |\n",
        "|-----------------------------------|------------------------------------------------|\n",
        "| Classification tasks              | Category index starting from zero              |\n",
        "| Amino Acid Classification tasks   | A list of category indices for each amino acid |\n",
        "| Regression tasks                  | Numerical values                               |\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/label_format.png?raw=true\" height=\"300\" width=\"800px\" align=\"center\">\n",
        "<br>\n",
        "\n",
        "- `stage` indicate whether the sample is used for training, validation, or testing. Ensure your dataset includes samples for all three stages. The values are: `train`, `valid`, `test`.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "\n",
        "1. Examples are available at /content/saprot/upload_files. Download to review their format, and then upload them for a trial.\n",
        "\n",
        "2. <a href=\"#fa2csv\">Here</a> you can convert your .fa/.fasta file to a .csv file, which corresponds to the data format for Multiple AA Sequences.\n",
        "\n",
        "3. <a href=\"#split_dataset\">Here</a> you can **randomly split your .csv dataset**, which means to add a `stage` column, where the ratio of `train`:`valid`:`test` is 8:1:1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vqdmLslQBI7e"
      },
      "outputs": [],
      "source": [
        "#@title 2.1: Task Config\n",
        "\n",
        "################################################################################\n",
        "################################## TASK CONFIG #################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "task_name = \"demo\" # @param {type:\"string\"}\n",
        "task_objective = \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\" # @param [\"Classify protein sequences (classification)\", \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\", \"Classify each Amino Acid (amino acid classification), e.g. Binding site detection\"]\n",
        "task_type = task_type_dict[task_objective]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'Enter the number of category in your training dataset here:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              max=1000000,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#################################### MODEL #####################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "##@markdown We use Parameter-Efficient Fine-Tuning Technique for model training. It enables us to store model weights in a small **adapter** without changing the original model weights during training. After training, you can get an adapter specific to your task.\n",
        "##@markdown As we use Parameter-Efficient Fine-Tuning Technique, which allows us to store model weights into an small adapter without adjusting the original model weights during training, it's necessary to specify both the original model and adapter for prediction.\n",
        "##@markdown\n",
        "##@markdown 1. Select a **base model** from the dropdown box `model_path` below.\n",
        "##@markdown\n",
        "##@markdown 2. If you want to **train on existing adapters**, check the box `use_your_data_to_train_on_an_existing_model` below. By running this cell, you will see an **adapter combobox**. We provide two ways to select your adapter:\n",
        "##@markdown  - Select a **Local Models** from the combobox.\n",
        "##@markdown   - Enter a **huggingface repository name** to the combobox. (e.g. \"SaProtAdapters/DeepLoc_cls10_35M\")\n",
        "##@markdown\n",
        "##@markdown You can also find some officical adapters in [here](https://huggingface.co/SaProtAdapters)\n",
        "base_model = \"Trained by yourself\" # @param [\"Official pretrained SaProt (35M)\", \"Official pretrained SaProt (650M)\", \"Trained by yourself\", \"Trained by peers\"]\n",
        "# base_model = \"westlake-repl/SaProt_35M_AF2\" # @param [\"westlake-repl/SaProt_35M_AF2\", \"westlake-repl/SaProt_650M_AF2\", \"Trained by yourself\", \"Trained by peers\"]\n",
        "# print(Fore.BLUE+f\"Model: {base_model}\"+Style.RESET_ALL)\n",
        "\n",
        "# use_your_data_to_train_on_an_existing_model = True # @param {type:\"boolean\"}\n",
        "if base_model == \"Official pretrained SaProt (35M)\":\n",
        "  base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "if base_model == \"Official pretrained SaProt (650M)\":\n",
        "  base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "if base_model in [\"Trained by yourself\", \"Trained by peers\"]:\n",
        "  use_your_data_to_train_on_an_existing_model = True\n",
        "else:\n",
        "  use_your_data_to_train_on_an_existing_model = False\n",
        "\n",
        "if use_your_data_to_train_on_an_existing_model:\n",
        "  if base_model == \"Trained by yourself\":\n",
        "    use_model_from = 'Local Models'\n",
        "  elif base_model == \"Trained by peers\":\n",
        "    use_model_from = 'SaProtHub Models'\n",
        "  adapter_combobox = select_adapter_from(use_model_from)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################### DATASET ####################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "\n",
        "data_type = \"Multiple AA Sequences\" # @param [\"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\", \"Huggingface Dataset\"]\n",
        "mode = \"Multiple Sequences\" if data_type in data_type_list[4:8] else \"Single Sequence\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##@markdown Complete some task configs and run this cell to Finetune SaProt on your dataset. <br>\n",
        "\n",
        "# def get_num_of_labels(selected_csv_dataset):\n",
        "#   df = pd.read_csv(selected_csv_dataset)\n",
        "#   num_of_labels = len(df['label'].unique())\n",
        "\n",
        "#   return num_of_labels\n",
        "\n",
        "\n",
        "##@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "############################### custom config ##################################\n",
        "################################################################################\n",
        "\n",
        "##@markdown ---\n",
        "##@markdown # <center>Training Task Config</center>\n",
        "\n",
        "\n",
        "# num_of_categories = 10 # @param {type:\"number\"}\n",
        "# #@markdown <font face=\"Consolas\" size=2 color='gray'>(Ignoring `num_of_categories` if predicting a value)\n",
        "\n",
        "\n",
        "  # print(Fore.BLUE+'It\\'s normal not to receive feedback once inputting is finished. Let\\'s move on to the next step.'+Style.RESET_ALL)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#@title 2.3: Select Model\n",
        "################################################################################\n",
        "\n",
        "# #@markdown We utilize **LoRA** (A Parameter-Efficient Fine-Tuning Technique), which allows us to store model weights into an small adapter without adjusting the original model weights during training.\n",
        "# #@markdown\n",
        "\n",
        "# #@markdown After training, you can obtain an adapter for your task.\n",
        "\n",
        "##@markdown ---\n",
        "##@markdown # <center>Model</center>\n",
        "\n",
        "\n",
        "\n",
        "# if use_your_data_to_train_on_an_existing_model:\n",
        "#   print(Fore.BLUE+f\"Loaded Adapter: {adapter_combobox.value}\"+Style.RESET_ALL)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fOcuVy_cBI7f"
      },
      "outputs": [],
      "source": [
        "#@title 2.2: Train your Model\n",
        "\n",
        "################################################################################\n",
        "############################## advance config ##################################\n",
        "################################################################################\n",
        "\n",
        "batch_size = 4 # @param [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\"] {type:\"raw\", allow-input: true}\n",
        "max_epochs = 1 # @param [\"10\", \"20\", \"50\"] {type:\"raw\", allow-input: true}\n",
        "learning_rate = 1.0e-3 # @param [\"1.0e-3\", \"5.0e-4\", \"1.0e-4\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "limit_train_batches=1.0\n",
        "limit_val_batches=1.0\n",
        "limit_test_batches=1.0\n",
        "\n",
        "val_check_interval=0.5\n",
        "\n",
        "use_lora = True\n",
        "num_workers = 2\n",
        "\n",
        "mask_struc_ratio=None\n",
        "# mask_struc_ratio=1.0\n",
        "\n",
        "download_adapter_to_your_computer = True\n",
        "\n",
        "################################################################################\n",
        "################################# MARKDOWM #####################################\n",
        "################################################################################\n",
        "\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'> `batch_size` depends on the number of training samples. If your training data set is large enough, we recommend using 32, 64,128,256, ..., others can be set to 8, 4, 2. (Note that you can not use a larger batch size if you the Colab default T4 GPU. Strongly suggest you subscribe to Colab Pro for an A100 GPU.)\n",
        "\n",
        "# #@markdown |  Recommended batch size   | T4  |  A100   |\n",
        "# #@markdown | ---                       | --- |  ---    |\n",
        "# #@markdown | SaProt_35M_AF2            |  4  |    16   |\n",
        "# #@markdown | SaProt_650M_AF2           |  -  |    8    |\n",
        "\n",
        "\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'>`max_epochs` refers to the maximum number of training iterations. A larger value needs more training time. The best model will be saved after each iteration.\n",
        "#@markdown You can adjust `max_epochs` to control training duration. (Note that the max running time of colab is 12hrs for unsubscribed user or 24hrs for Colab Pro+ user) <br>\n",
        "#@markdown\n",
        "\n",
        "# download_adapter_to_your_computer = True #@param {type:\"boolean\"}\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'>`learning_rate` affects the convergence speed of the model.\n",
        "#@markdown Through experimentation, we have found that `5.0e-4` is a good default value for base model `SaProt_650M_AF2` and `1.0e-3` for `SaProt_35M_AF2`.\n",
        "\n",
        "################################################################################\n",
        "################################# CONFIG #######################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "config.setting.run_mode = \"train\"\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# ADAPTER ######################################\n",
        "################################################################################\n",
        "config.model.kwargs.use_lora = use_lora\n",
        "\n",
        "if base_model in [\"Trained by yourself\", \"Trained by peers\"]:\n",
        "\n",
        "  adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value\n",
        "\n",
        "  if not adapter_path.exists():\n",
        "    snapshot_download(repo_id=adapter_combobox.value, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "  adapter_config = Path(adapter_path) / \"adapter_config.json\"\n",
        "  with open(adapter_config, 'r') as f:\n",
        "    base_model = json.load(f)['base_model_name_or_path']\n",
        "\n",
        "  config.model.kwargs.lora_config_path = adapter_path\n",
        "\n",
        "else:\n",
        "  config.model.kwargs.lora_config_path = None\n",
        "\n",
        "if config.setting.run_mode == 'train':\n",
        "  config.model.kwargs.lora_inference = False\n",
        "if config.setting.run_mode == 'test':\n",
        "  config.model.kwargs.lora_inference = True\n",
        "\n",
        "################################################################################\n",
        "################################# MODEL ########################################\n",
        "################################################################################\n",
        "\n",
        "if task_type in [\"classification\", \"token_classification\"]:\n",
        "  # config.model.kwargs.num_labels = get_num_of_labels(selected_csv_dataset)\n",
        "  config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "\n",
        "\n",
        "config.model.model_py_path = model_type_dict[task_type]\n",
        "\n",
        "config.model.kwargs.config_path = base_model\n",
        "config.dataset.kwargs.tokenizer = base_model\n",
        "\n",
        "config.model.save_path = str(ADAPTER_HOME / f\"{task_type}\" / \"Local\" / f\"{task_name}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# DATASET ######################################\n",
        "################################################################################\n",
        "\n",
        "if data_type == data_type_list[8]:\n",
        "  lmdb_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "else:\n",
        "  csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "  from saprot.utils.construct_lmdb import construct_lmdb\n",
        "  construct_lmdb(csv_dataset_path, LMDB_HOME, task_name, task_type)\n",
        "  lmdb_dataset_path = LMDB_HOME / task_name\n",
        "\n",
        "\n",
        "config.dataset.dataset_py_path = dataset_type_dict[task_type]\n",
        "\n",
        "config.dataset.train_lmdb = str(lmdb_dataset_path / \"train\")\n",
        "config.dataset.valid_lmdb = str(lmdb_dataset_path / \"valid\")\n",
        "config.dataset.test_lmdb = str(lmdb_dataset_path / \"test\")\n",
        "\n",
        "#  batch size, num_workers\n",
        "config.dataset.dataloader_kwargs.batch_size = batch_size\n",
        "config.dataset.dataloader_kwargs.num_workers = num_workers\n",
        "\n",
        "# mask_struc\n",
        "# config.dataset.kwargs.mask_struc_ratio= mask_struc_ratio\n",
        "\n",
        "################################################################################\n",
        "############################## TRAINER #########################################\n",
        "################################################################################\n",
        "\n",
        "config.Trainer.accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config.Trainer.accumulate_grad_batches= int(64 / batch_size)\n",
        "\n",
        "# epoch\n",
        "config.Trainer.max_epochs = max_epochs\n",
        "# test only: load the existing model\n",
        "if config.Trainer.max_epochs == 0:\n",
        "  config.model.save_path = config.model.kwargs.lora_config_path\n",
        "\n",
        "# learning rate\n",
        "config.model.lr_scheduler_kwargs.init_lr = learning_rate\n",
        "\n",
        "# trainer\n",
        "config.Trainer.limit_train_batches=limit_train_batches\n",
        "config.Trainer.limit_val_batches=limit_val_batches\n",
        "config.Trainer.limit_test_batches=limit_test_batches\n",
        "config.Trainer.val_check_interval=val_check_interval\n",
        "\n",
        "# strategy\n",
        "strategy = {\n",
        "    # - deepspeed\n",
        "    # 'class': 'DeepSpeedStrategy',\n",
        "    # 'stage': 2\n",
        "\n",
        "    # - None\n",
        "    # 'class': None,\n",
        "\n",
        "    # - DP\n",
        "    # 'class': 'DataParallelStrategy',\n",
        "\n",
        "    # - DDP\n",
        "    # 'class': 'DDPStrategy',\n",
        "    # 'find_unused_parameter': True\n",
        "}\n",
        "config.Trainer.strategy = strategy\n",
        "\n",
        "################################################################################\n",
        "############################## CONFIG ##########################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################## Run the task ####################################\n",
        "################################################################################\n",
        "\n",
        "print('='*100)\n",
        "print(Fore.BLUE+f\"Training task type: {task_type}\"+Style.RESET_ALL)\n",
        "print(Fore.BLUE+f\"Dataset: {lmdb_dataset_path}\"+Style.RESET_ALL)\n",
        "print(Fore.BLUE+f\"Base Model: {config.model.kwargs.config_path}\"+Style.RESET_ALL)\n",
        "if use_your_data_to_train_on_an_existing_model:\n",
        "  print(Fore.BLUE+f\"Existing model: {config.model.kwargs.lora_config_path}\"+Style.RESET_ALL)\n",
        "print('='*100)\n",
        "\n",
        "from saprot.scripts.training import finetune\n",
        "print(config)\n",
        "finetune(config)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################## Save the adapter ################################\n",
        "################################################################################\n",
        "\n",
        "print(Fore.BLUE)\n",
        "print(f\"Model is saved to \\\"{config.model.save_path}\\\" on colab Server\")\n",
        "print(Style.RESET_ALL)\n",
        "\n",
        "if download_adapter_to_your_computer:\n",
        "  adapter_zip = Path(config.model.save_path) / f\"{task_name}.adapter.zip\"\n",
        "  !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"adapter_model.bin\" \"README.md\"\n",
        "  # with zipfile.ZipFile(adapter_zip, 'w') as zipf:\n",
        "  #   zip_files = [str(file_path) for file_path in Path(config.model.save_path).glob(\"*\")]\n",
        "  #   print(zip_files)\n",
        "  #   for file in zip_files:\n",
        "  #     zipf.write(file, Path(file).name)\n",
        "\n",
        "  print(\"Downloading adapter to your local computer\")\n",
        "  if adapter_zip.exists():\n",
        "    files.download(adapter_zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UnKX1BTZBI7f"
      },
      "outputs": [],
      "source": [
        "#@title **2.3: Login HuggingFace to upload your model (Optional)**\n",
        "################################################################################\n",
        "###################### Login HuggingFace #######################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6XlluTsPBI7m"
      },
      "outputs": [],
      "source": [
        "#@title **2.4: Upload Model (Optional)**\n",
        "\n",
        "# #@markdown Your Huggingface adapter repository names follow the format `<username>/<task_name>`.\n",
        "\n",
        "################################################################################\n",
        "########################## Metadata  ###########################################\n",
        "################################################################################\n",
        "#@markdown You can add some description to your model.\n",
        "model_name = \"task_demo\" # @param {type:\"string\"}\n",
        "description = \"This model is used for a demo task\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown For the classification model, please provide detailed information about the meanings of all labels.\n",
        "\n",
        "#@markdown For example, in a Subcellular Localization Classification Task with 10 categories, label=0 means the protein is located in the Nucleus, label=1 means the protein is located in the Cytoplasm, and so on. The information should be provided as follows:\n",
        "\n",
        "#@markdown `Nucleus, Cytoplasm, Extracellular, Mitochondrion, Cell.membrane, Endoplasmic.reticulum, Plastid, Golgi.apparatus, Lysosome/Vacuole, Peroxisome`\n",
        "\n",
        "\n",
        "# #@markdown > 0: Nucleus <br>\n",
        "# #@markdown > 1: Cytoplasm <br>\n",
        "# #@markdown > 2: Extracellular <br>\n",
        "# #@markdown > ... <br>\n",
        "# #@markdown > 9: Peroxisome <br>\n",
        "\n",
        "label_meanings = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### Move Files  ########################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import HfApi, Repository, ModelFilter\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "user = api.whoami()\n",
        "\n",
        "if model_name == \"\":\n",
        "  model_name = task_name\n",
        "repo_name = user['name'] + '/' + model_name\n",
        "local_dir = Path(\"/content/saprot/model_to_push\") / repo_name\n",
        "local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_list = [repo.id for repo in api.list_models(filter=ModelFilter(author=user['name']))]\n",
        "if repo_name not in repo_list:\n",
        "  api.create_repo(repo_name, private=False)\n",
        "\n",
        "repo = Repository(local_dir=local_dir, clone_from=repo_name)\n",
        "\n",
        "command = f\"cp {config.model.save_path}/* {local_dir}/\"\n",
        "subprocess.run(command, shell=True)\n",
        "\n",
        "################################################################################\n",
        "########################## Modify README  ######################################\n",
        "################################################################################\n",
        "import json\n",
        "\n",
        "md_path = local_dir / \"README.md\"\n",
        "\n",
        "if task_type in [\"classification\", \"token_classification\"]:\n",
        "  label_meanings_md = ''\n",
        "  for index, label in enumerate(label_meanings.split(', ')):\n",
        "    label_meanings_md += f\"{index}: {label} <br> \"\n",
        "\n",
        "  # print(label_meanings_md)\n",
        "  description = description + \"<br><br> The digital label means: <br>\" + label_meanings_md\n",
        "\n",
        "replace_data = {\n",
        "    \"<!-- Provide a quick summary of what the model is/does. -->\": description\n",
        "}\n",
        "\n",
        "with open(md_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "for key, value in replace_data.items():\n",
        "    if value != \"\":\n",
        "        content = content.replace(key, value)\n",
        "\n",
        "# new_md_path = \"README.md\"\n",
        "with open(md_path, \"w\") as file:\n",
        "    file.write(content)\n",
        "\n",
        "################################################################################\n",
        "########################## Upload Model  #######################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "repo.push_to_hub(commit_message=\"Upload adapter model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ1JgmrsBI7m"
      },
      "source": [
        "# **3: Use SaProt to Predict**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04hq3Z6QBI7m"
      },
      "source": [
        "## 3.1: Classification&Regression Prediction\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "For the prediction dataset, **only** `Sequence` column is required in the CSV file.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_Sequences_data_format.png?raw=true\" height=\"200\" width=\"800px\" align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_PDB_CIF_Structures_data_format.png?raw=true\" height=\"200\" width=\"500px\" align=\"center\">\n",
        "\n",
        "You can refer to the <a href='#data_format'>instruction</a> for detailed data formats.\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m9oB-TRJBI7m"
      },
      "outputs": [],
      "source": [
        "#@title 3.1.1: Task Config\n",
        "\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "################################################################################\n",
        "################################# TASK #########################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "\n",
        "task_objective = \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\" # @param [\"Classify protein sequences (classification)\", \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\", \"Classify each Amino Acid (amino acid classification), e.g. Binding site detection\"]\n",
        "task_type = task_type_dict[task_objective]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'The number of categories in your classification task:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              # max=10,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################## MODEL #######################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "##@markdown As we use Parameter-Efficient Fine-Tuning Technique, which allows us to store model weights into an small adapter without adjusting the original model weights during training, it's necessary to specify both the original model and adapter for prediction.\n",
        "##@markdown\n",
        "##@markdown 1. Select a **base model**\n",
        "##@markdown\n",
        "##@markdown 2. By running this cell, you will see an **model combobox**. We provide two ways to select your adapter:\n",
        "##@markdown  - Select a **local model** from the combobox.\n",
        "##@markdown   - Enter a **huggingface repository name** to the combobox. (e.g. \"SaProtHub/DeepLoc_cls10_35M\")\n",
        "##@markdown\n",
        "##@markdown You can also find some officical adapters in [here](https://huggingface.co/SaProtHub)\n",
        "# base_model = \"westlake-repl/SaProt_35M_AF2\" #@param ['westlake-repl/SaProt_35M_AF2', 'westlake-repl/SaProt_650M_AF2'] {allow-input:true}\n",
        "use_model_from = \"Local Models\" # @param [\"Local Models\", \"SaProtHub Models\"]\n",
        "\n",
        "# use_existing_model = True # @param {type:\"boolean\"}\n",
        "# use_existing_model = True\n",
        "# if use_existing_model:\n",
        "#   adapter_combobox = select_adapter()\n",
        "\n",
        "adapter_input = select_adapter_from(use_model_from)\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "# # @markdown Please ensure that the selected task type aligns with the training task type of the model you intend to utilize.\n",
        "\n",
        "## @markdown If you are conducting inference on a classification task, please ensure that the `num_of_category` matches the number of categories in the training dataset. Otherwise, you do not need to assign `num_of_category`.\n",
        "\n",
        "\n",
        "##@markdown You have two options to provide your protein sequences:\n",
        "##@markdown - **Single Sequence: Enter a single SA sequence** into the input box, you can get a SA Sequence by clicking <a href=\"#get_SA_seq\">here</a>\n",
        "##@markdown - **Multiple Sequences: Select a dataset**, you can upload a dataset from <a href=\"#upload_dataset\">here</a>\n",
        "\n",
        "\n",
        "##@markdown <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/InferenceFileFormat.png\" height=\"256\" align=\"center\" style=\"height:256px\">\n",
        "\n",
        "\n",
        "# print(Fore.BLUE+f\"Data type: {data_type}\"+Style.RESET_ALL)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################ DATASET #######################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "data_type = \"Multiple PDB/CIF Structures\" # @param [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "mode = \"Multiple Sequences\" if data_type in data_type_list[4:8] else \"Single Sequence\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "\n",
        "#@markdown <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "85bPMEjUBI7n"
      },
      "outputs": [],
      "source": [
        "#@title 3.1.2: Get your Result\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "import sys\n",
        "from saprot.scripts.training import my_load_model\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# 0. MARKDOWN ##################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "# @markdown Click the run button to make prediction.\n",
        "\n",
        "# @markdown <font color=\"red\">**Note that:**</font> When predicting a category, the index of categories starts from zero.\n",
        "\n",
        "################################################################################\n",
        "################################# 1. DATASET ##################################\n",
        "################################################################################\n",
        "\n",
        "if mode == \"Multiple Sequences\":\n",
        "  csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "else:\n",
        "  single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# 2. MODEL ##################################\n",
        "################################################################################\n",
        "# base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "if use_model_from == \"SaProtHub Models\":\n",
        "  snapshot_download(repo_id=adapter_input.value, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / adapter_input.value)\n",
        "\n",
        "adapter_path = ADAPTER_HOME / task_type / adapter_input.value\n",
        "adapter_config = Path(adapter_path) / \"adapter_config.json\"\n",
        "with open(adapter_config, 'r') as f:\n",
        "  base_model = json.load(f)['base_model_name_or_path']\n",
        "\n",
        "\n",
        "# if use_existing_model:\n",
        "#   if adapter_combobox.value =='':\n",
        "#     print(\"Please select a model!\")\n",
        "#     sys.exit()\n",
        "\n",
        "#   if \". \" in adapter_combobox.value:\n",
        "#     adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value\n",
        "#   else:\n",
        "#     adapter_path = adapter_combobox.value\n",
        "\n",
        "################################################################################\n",
        "##################################### config ###################################\n",
        "################################################################################\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "# task\n",
        "if task_type in [ \"classification\", \"token_classification\"]:\n",
        "  # config.model.kwargs.num_labels = num_of_categories.value\n",
        "  config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "# base model\n",
        "config.model.model_py_path = model_type_dict[task_type]\n",
        "# config.model.save_path = model_save_path\n",
        "config.model.kwargs.config_path = base_model\n",
        "\n",
        "# lora\n",
        "config.model.kwargs.lora_config_path = adapter_path\n",
        "config.model.kwargs.use_lora = True\n",
        "config.model.kwargs.lora_inference = True\n",
        "\n",
        "################################################################################\n",
        "################################### inference ##################################\n",
        "################################################################################\n",
        "from peft import PeftModelForSequenceClassification\n",
        "\n",
        "model = my_load_model(config.model)\n",
        "tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "# print(\"#\"*100)\n",
        "print(Fore.BLUE+f\"Inference task type: {task_type}\"+Style.RESET_ALL)\n",
        "if mode == \"Multiple Sequences\":\n",
        "  print(Fore.BLUE+f\"Dataset: {csv_dataset_path}\"+Style.RESET_ALL)\n",
        "else:\n",
        "  print(Fore.BLUE+f\"Dataset: {raw_data}\"+Style.RESET_ALL)\n",
        "\n",
        "print(Fore.BLUE+f\"Model: {base_model} - {adapter_path}\"+Style.RESET_ALL)\n",
        "# if use_existing_model:\n",
        "#   print(Fore.BLUE+f\"Adapter: {adapter_path}\"+Style.RESET_ALL)\n",
        "\n",
        "outputs_list=[]\n",
        "\n",
        "if mode == \"Multiple Sequences\":\n",
        "  timestamp = str(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "  output_file = OUTPUT_HOME / f'output_{timestamp}.csv'\n",
        "  df = pd.read_csv(csv_dataset_path)\n",
        "  for index in tqdm(range(len(df))):\n",
        "    seq = df['Sequence'].iloc[index]\n",
        "    inputs = tokenizer(seq, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model(inputs)\n",
        "    outputs_list.append(outputs)\n",
        "\n",
        "  df['score'] = [output.cpu().tolist() for output in outputs_list]\n",
        "  df.to_csv(output_file, index=False)\n",
        "  files.download(output_file)\n",
        "\n",
        "  print(Fore.BLUE+f\"\\nThe prediction result is saved to {output_file} and your local computer.\"+Style.RESET_ALL)\n",
        "\n",
        "else:\n",
        "  # print(\"You are making inference based on a sequence that you entered\")\n",
        "  inputs = tokenizer(single_sa_seq, return_tensors=\"pt\")\n",
        "  inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "  outputs = model(inputs)\n",
        "  outputs_list.append(outputs)\n",
        "\n",
        "################################################################################\n",
        "##################################### output ###################################\n",
        "################################################################################\n",
        "\n",
        "print()\n",
        "print(\"#\"*100)\n",
        "print(Fore.BLUE+\"outputs:\"+Style.RESET_ALL)\n",
        "\n",
        "if task_type == \"classification\":\n",
        "  import torch.nn.functional as F\n",
        "  softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "  for index, output in enumerate(softmax_output_list):\n",
        "    print(f\"For Sequence {index}, Prediction: Category {output.index(max(output))}, Probability: {output}\")\n",
        "elif task_type == \"regression\":\n",
        "  output_list = [output.squeeze().tolist() for output in outputs_list]\n",
        "  for index, output in enumerate(outputs_list):\n",
        "    print(f\"For Sequence {index}, Prediction: Value {output.item()}\")\n",
        "elif task_type == \"token_classification\":\n",
        "  import torch.nn.functional as F\n",
        "  softmax_output_list = [F.softmax(output, dim=-1).squeeze().tolist() for output in outputs_list]\n",
        "  # print(softmax_output_list)\n",
        "  print(\"The probability of each category:\")\n",
        "  for seq_index, seq in enumerate(softmax_output_list):\n",
        "    seq_prob_df = pd.DataFrame(seq)\n",
        "    print('='*100)\n",
        "    print(f'Sequence {seq_index + 1}:')\n",
        "    print(seq_prob_df[1:-1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdMIr07vBI7n"
      },
      "source": [
        "## 3.2: Mutational Effect Prediction\n",
        "\n",
        "<br>\n",
        "\n",
        "### Mutation Task\n",
        "- Single-site or Multi-site mutagenesis\n",
        "- Saturation mutagenesis\n",
        "\n",
        "<br>\n",
        "\n",
        "### Mutation Dataset\n",
        "\n",
        "For `Single-site or Multi-site mutagenesis`, **one additional column** are required in the CSV file: `mutation`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_AA_Sequences_data_format_mutation.png\n",
        "?raw=true\" height=\"200\" width=\"500px\" align=\"center\">\n",
        "\n",
        "- `mutation` column contains the **mutation information**.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Mutation Information\n",
        "\n",
        "Here is the detail about the representation of **mutation information**: <a name=\"mutation info\"></a>\n",
        "\n",
        "| mode | mutation information|\n",
        "| --- | --- |\n",
        "| Single-site mutagenesis | H87Y |\n",
        "| Multi-site mutagenesis | H87Y:V162M:P179L:P179R |\n",
        "\n",
        "- For `Single-site mutagenesis`, we use a term like \"H87Y\" to denote the mutation, where the first letter represents the **original amino acid**, the number in the middle represents the **mutation site** (indexed starting from 1), and the last letter represents the **mutated amino acid**,\n",
        "- For `Multi-site mutagenesis`, we use a colon \":\" to connect each single-site mutations, such as \"H87Y:V162M:P179L:P179R\".\n",
        "\n",
        "<!-- ### Prediction Result -->\n",
        "\n",
        "\n",
        "<!-- ### How to use your model for Mutational Effect Prediction -->\n",
        "\n",
        "<!--## 1. Input and Output\n",
        "\n",
        " You have four different combinations of **mutation task** and **mode** to choose from: -->\n",
        "\n",
        "<!--\n",
        " |Combination| Input | Output |\n",
        " | --- | --- | --- |\n",
        " |`Single-site or Multi-site mutagenesis` + `Single Sequence`| Enter **a SA sequence** and **a mutation information**| a score of the mutation |\n",
        " |`Single-site or Multi-site mutagenesis` + `Multiple Sequences`| Select **a dataset** and upload **a .csv file containing mutation information**| a .csv file containing the scores of mutations |\n",
        " |`Saturation mutagenesis` + `Single Sequence`| Enter **a SA sequence**| a .csv file containing the scores of all mutation on every position of the sequence |\n",
        " |`Saturation mutagenesis` + `Multiple Sequences`| Select **a dataset**| a .zip file containing the .csv files of the Saturation mutagenesis on every sequence |\n",
        "  -->\n",
        "\n",
        " <!-- <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/mutation_input_output.png\" height=\"500\" width=\"800px\" align=\"center\"> -->\n",
        "\n",
        "<!-- ### 2. Format of the uploaded .csv file containing mutation information\n",
        "\n",
        "For Multiple Sequences, you are required to **upload an additional .csv file** as your mutation information.\n",
        "<font color=red>Please ensure that each mutation in the mutation CSV file corresponds to each Sequence in the dataset CSV file.</font>\n",
        " <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/MutationFormat.png\" height=\"256\" align=\"center\" style=\"height:256px\"> -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uxD_KOF1BI7n"
      },
      "outputs": [],
      "source": [
        "#@title 3.2.1: Task Config\n",
        "\n",
        "mutation_task = \"Single-site or Multi-site mutagenesis\" #@param [\"Single-site or Multi-site mutagenesis\", \"Saturation mutagenesis\"]\n",
        "\n",
        "data_type = \"Single AA Sequence\" # @param [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "mode = \"Multiple Sequences\" if data_type in data_type_list[4:8] else \"Single Sequence\"\n",
        "\n",
        "if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "  if mode == \"Single Sequence\":\n",
        "    input_mut = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder='Enter Single Mutation Information here',\n",
        "      # description='SA Sequence:',\n",
        "      disabled=False)\n",
        "    print(Fore.BLUE+\"Mutation:\"+Style.RESET_ALL)\n",
        "    input_mut.layout.width = '500px'\n",
        "    display(input_mut)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3JSk91jmBI7n"
      },
      "outputs": [],
      "source": [
        "#@title 3.2.2: Get your Result\n",
        "\n",
        "################################################################################\n",
        "################################# DATASET ###################################\n",
        "################################################################################\n",
        "if mode == \"Single Sequence\":\n",
        "  seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "else:\n",
        "  dataset_csv_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "################################################################################\n",
        "################################# Task Info ####################################\n",
        "################################################################################\n",
        "base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "\n",
        "print(Fore.BLUE)\n",
        "print(f\"Mutation task: {mutation_task}\")\n",
        "print(f\"Mode: {mode}\")\n",
        "print(f\"Model: {base_model}\")\n",
        "if mode == \"Multiple Sequences\":\n",
        "  print(Fore.BLUE+f\"Dataset: {dataset_csv_path}\"+Style.RESET_ALL)\n",
        "else:\n",
        "  print(Fore.BLUE+f\"Dataset: {seq}\"+Style.RESET_ALL)\n",
        "\n",
        "print(Style.RESET_ALL)\n",
        "\n",
        "print(f\"Predicting...\")\n",
        "timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "\n",
        "################################################################################\n",
        "################################# load model ###################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.model.saprot.saprot_foldseek_mutation_model import SaprotFoldseekMutationModel\n",
        "\n",
        "config = {\n",
        "    \"foldseek_path\": None,\n",
        "    \"config_path\": base_model,\n",
        "    \"load_pretrained\": True,\n",
        "}\n",
        "model = SaprotFoldseekMutationModel(**config)\n",
        "tokenizer = model.tokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### Single Sequence ####################################\n",
        "################################################################################\n",
        "if mode == \"Single Sequence\":\n",
        "\n",
        "  if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "    mut = input_mut.value\n",
        "    score = model.predict_mut(seq, mut)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "    print(f\"The score of mutation {mut} is {Fore.BLUE}{score}{Style.RESET_ALL}\")\n",
        "\n",
        "  if mutation_task==\"Saturation mutagenesis\":\n",
        "    timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "    output_path = OUTPUT_HOME / f'{timestamp}_prediction_output.csv'\n",
        "\n",
        "    mut_dicts = []\n",
        "    for pos in range(1, int(len(seq) / 2)+1):\n",
        "      mut_dict = model.predict_pos_mut(seq, pos)\n",
        "      mut_dicts.append(mut_dict)\n",
        "\n",
        "    mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "    df = pd.DataFrame(mut_list)\n",
        "    df.to_csv(output_path, index=None)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "    files.download(output_path)\n",
        "    print(f\"\\n{Fore.BLUE}The result has been saved to {output_path} and your local computer.{Style.RESET_ALL}\")\n",
        "\n",
        "################################################################################\n",
        "########################### Multiple Sequences #################################\n",
        "################################################################################\n",
        "if mode == \"Multiple Sequences\":\n",
        "\n",
        "  dataset_df = pd.read_csv(dataset_csv_path)\n",
        "  results = []\n",
        "\n",
        "  if mutation_task==\"Single-site or Multi-site mutagenesis\":\n",
        "    for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "     seq = row['Sequence']\n",
        "     mut_info = row['mutation']\n",
        "     results.append(model.predict_mut(seq, mut_info))\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "\n",
        "    # result_df = pd.DataFrame()\n",
        "    # result_df['Sequence'] = dataset_df['Sequence']\n",
        "    # result_df['mutation'] = dataset_df['mutation']\n",
        "    dataset_df['score'] = results\n",
        "\n",
        "    output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_csv_path).stem}.csv\"\n",
        "    dataset_df.to_csv(output_path, index=None)\n",
        "    files.download(output_path)\n",
        "    print(f\"{Fore.BLUE}The result has been saved to {output_path} and your local computer {Style.RESET_ALL}\")\n",
        "\n",
        "  else:\n",
        "    for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "      seq = row['Sequence']\n",
        "      mut_dicts = []\n",
        "      for pos in range(1, int(len(seq) / 2)+1):\n",
        "        mut_dict = model.predict_pos_mut(seq, pos)\n",
        "        mut_dicts.append(mut_dict)\n",
        "      mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "      result_df = pd.DataFrame(mut_list)\n",
        "      results.append(result_df)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "\n",
        "    zip_files = []\n",
        "    for i in range(len(results)):\n",
        "      output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_csv_path).stem}_Sequence{i+1}.csv\"\n",
        "      results[i].to_csv(output_path, index=None)\n",
        "      zip_files.append(output_path)\n",
        "\n",
        "    # zip and download zip to local computer\n",
        "    zip_path = OUTPUT_HOME / f\"{timestamp}_{Path(dataset_csv_path).stem}.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "        for file in zip_files:\n",
        "            zipf.write(file, os.path.basename(file))\n",
        "    files.download(zip_path)\n",
        "    print(f\"{Fore.BLUE}The result has been saved to {zip_path} and your local computer{Style.RESET_ALL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlQdqTcBI7n"
      },
      "source": [
        "## 3.3: Inverse Folding Prediction\n",
        "\n",
        "Predict the amino acid sequence from protein backbone structure.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The protein backbone structure should be provided in .pdb/.cif file format.\n",
        "\n",
        "<br>\n",
        "\n",
        "<!-- Predict the residue sequence of a structure-aware sequence with masked amino acids (which could be all masked or partially masked).\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Enter a **SA sequence with masked amino acids** into the `sa_seq` input box.\n",
        "\n",
        "<br>\n",
        "\n",
        "For example,\n",
        "**input** is a SA Sequence with masked amino acids:\n",
        "\n",
        "`#d#v#v#v#p#p#p#p#a#p#a#q#k#k#k#k#w`\n",
        "\n",
        "and the **output** predicted by model is an AA Sequence:\n",
        "\n",
        "`MEELGLPDLPPGGVVVV`.\n",
        "\n",
        "<br> -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DT7M_DU2BI7n"
      },
      "outputs": [],
      "source": [
        "#@title 3.3.1: Upload .pdb/.cif structure file\n",
        "\n",
        "#@markdown After clicking the run button, an upload button will appear for you to upload your .pdb/.cif structure file.\n",
        "\n",
        "#@markdown After uploading is finished, the .pdb/.cif structure will be transformed into the corresponding Amino Acid Sequence and Structure (3Di) Sequence.\n",
        "\n",
        "#@markdown You can **mask specific amino acids** in the AA sequence with '#' at certain positions, allowing the model to make predictions for those masked amino acids.\n",
        "\n",
        "data_type = \"Single PDB/CIF Structure\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "aa_seq = sa_seq[0::2]\n",
        "struc_seq = sa_seq[1::2]\n",
        "\n",
        "# masked_sa_seq = ''\n",
        "# for s in sa_seq[1::2]:\n",
        "#   masked_sa_seq += '#' + s\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "input_aa_seq = ipywidgets.Text(\n",
        "      value=aa_seq,\n",
        "      placeholder='Enter Amino Acid Sequence here',\n",
        "      disabled=False)\n",
        "print(Fore.BLUE+\"Amino Acid Sequence:\"+Style.RESET_ALL)\n",
        "input_aa_seq.layout.width = '500px'\n",
        "display(input_aa_seq)\n",
        "\n",
        "input_struc_seq = ipywidgets.Text(\n",
        "  value=struc_seq,\n",
        "  placeholder='Enter Structure Sequence here',\n",
        "  disabled=False)\n",
        "print(Fore.BLUE+\"Structure Sequence:\"+Style.RESET_ALL)\n",
        "input_struc_seq.layout.width = '500px'\n",
        "display(input_struc_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x5SgXe0a9FQQ"
      },
      "outputs": [],
      "source": [
        "#@title 3.3.2: Predict Amino Acid Sequence\n",
        "\n",
        "#@markdown Click the run button to get the Amino Acid Sequence\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### Dataset ########################################\n",
        "################################################################################\n",
        "\n",
        "masked_aa_seq = input_aa_seq.value\n",
        "masked_struc_seq = input_struc_seq.value\n",
        "masked_sa_seq = ''.join(a + b for a, b in zip(masked_aa_seq, masked_struc_seq))\n",
        "\n",
        "################################################################################\n",
        "############################### Model ##########################################\n",
        "################################################################################\n",
        "base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "config = {\n",
        "    \"config_path\": base_model,\n",
        "    \"load_pretrained\": True,\n",
        "}\n",
        "from saprot.model.esm.saprot_if_model import SaProtIFModel\n",
        "model = SaProtIFModel(**config)\n",
        "tokenizer = model.tokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "################################################################################\n",
        "############################### Predict ########################################\n",
        "################################################################################\n",
        "\n",
        "pred_aa_seq = model.predict(masked_sa_seq)\n",
        "\n",
        "print(\"#\"*100)\n",
        "print(Fore.BLUE+\"outputs:\"+Style.RESET_ALL)\n",
        "print(pred_aa_seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIIoTQgJBI7o"
      },
      "source": [
        "# **4: (Optional) Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYjzMSqaY6SD"
      },
      "source": [
        "## 4.1: Get Structure-Aware Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "BgsBSLcmBI7o"
      },
      "outputs": [],
      "source": [
        "#@title 4.1.1: Input\n",
        "\n",
        "################################################################################\n",
        "################################ input #########################################\n",
        "################################################################################\n",
        "data_type = \"Multiple PDB/CIF Structures\" # @param [\"Single AA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "mode = \"Multiple Sequences\" if data_type in data_type_list[4:8] else \"Single Sequence\"\n",
        "\n",
        "if data_type == data_type_list[7]:\n",
        "    # upload and unzip PDB files\n",
        "    print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "    pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    if pdb_zip_path.suffix != \".zip\":\n",
        "      logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "      raise RuntimeError(\"The data type does not match.\")\n",
        "    print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "      file_names = zip_ref.namelist()\n",
        "      zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "\n",
        "    uploaded_csv_path = UPLOAD_FILE_HOME / f\"{pdb_zip_path.stem}.csv\"\n",
        "    df = pd.DataFrame(file_names, columns=['Sequence'])\n",
        "    df.to_csv(uploaded_csv_path, index=False)\n",
        "    raw_data = uploaded_csv_path\n",
        "\n",
        "else:\n",
        "  raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "# input_raw_data_by_data_type -> raw_data -> get_SA_sequence_by_data_type -> single_input_seq / csv_dataset_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_W3lJ-yDBI7o"
      },
      "outputs": [],
      "source": [
        "#@title 4.1.2: Output\n",
        "#@markdown Click the run button to get the SA Sequence.\n",
        "sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "if mode == \"Single Sequence\":\n",
        "  print(f\"Amino Acid Sequence: {sa_seq[0::2]}\")\n",
        "  print(f\"Structure Sequence: {sa_seq[1::2]}\")\n",
        "  print(\"=\"*100)\n",
        "  print(Fore.BLUE  + \"The Structure-Aware Sequence is here, double click to select and copy it:\" + Style.RESET_ALL)\n",
        "  print(sa_seq)\n",
        "\n",
        "elif mode == \"Multiple Sequences\":\n",
        "  print(Fore.BLUE  + \"The Structure-Aware Sequences are saved in a .csv file here:\" + Style.RESET_ALL)\n",
        "  print(sa_seq)\n",
        "  files.download(sa_seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAE5-b_iZEyq"
      },
      "source": [
        "## 4.2: Convert `.fa/.fasta` file to `.csv` file in the data format of \"Multiple AA Sequences\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IDkm_OeABI7o"
      },
      "outputs": [],
      "source": [
        "#@title 4.2.1: `.fa/.fasta` -> Multiple AA Sequences `.csv` <a name=\"fa2csv\"></a>\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "\n",
        "aa_seq_dict = { \"Sequence\": [],\n",
        "                # \"label\": [],\n",
        "                # \"stage\":[]\n",
        "                }\n",
        "\n",
        "fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "assert Path(fa_file_path).name.split('.')[1] in ['fa', 'fasta'], \"Please upload a .fa or .fasta file.\"\n",
        "with fa_file_path.open(\"r\") as fa:\n",
        "  for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "      aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "\n",
        "fa_df = pd.DataFrame(aa_seq_dict)\n",
        "print(fa_df[5:])\n",
        "\n",
        "csv_file_path = f'/content/saprot/upload_files/{fa_file_path.stem}.csv'\n",
        "fa_df.to_csv(csv_file_path, index=None)\n",
        "files.download(csv_file_path)\n",
        "\n",
        "################################################################################\n",
        "############################ .fa 2 .csv and split ##############################\n",
        "################################################################################\n",
        "\n",
        "# automatically_split_dataset = False # @param {type:\"boolean\"}\n",
        "# split = ['train', 'valid', 'test']\n",
        "\n",
        "# aa_seq_dict = { \"Sequence\": [],\n",
        "#                 \"label\": [],\n",
        "#                 \"stage\":[]}\n",
        "\n",
        "\n",
        "\n",
        "# if automatically_split_dataset:\n",
        "\n",
        "#   fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#   label = fa_file_path.stem\n",
        "\n",
        "#   with fa_file_path.open(\"r\") as fa:\n",
        "#       for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "#           aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "#           aa_seq_dict[\"label\"].append(label)\n",
        "#   weights = [0.8, 0.1, 0.1]\n",
        "#   aa_seq_dict[\"stage\"] = np.random.choice(split, size=len(aa_seq_dict[\"Sequence\"]), p=weights).tolist()\n",
        "\n",
        "# else:\n",
        "#   for i in range(3):\n",
        "#     print(Fore.BLUE+f\"Please upload a .fa file as your {split[i]} dataset\")\n",
        "#     fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#     label = fa_file_path.stem\n",
        "\n",
        "#     with fa_file_path.open(\"r\") as fa:\n",
        "#         for record in tqdm(SeqIO.parse(fa, 'fasta')):\n",
        "#             aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "#             aa_seq_dict[\"label\"].append(label)\n",
        "#             aa_seq_dict[\"stage\"].append(split[i])\n",
        "\n",
        "#     print()\n",
        "#     print(\"=\"*100)\n",
        "\n",
        "# fa_df = pd.DataFrame(aa_seq_dict)\n",
        "# timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "# fa_df.to_csv(f'/content/saprot/upload_files/{timestamp}.csv', index=None)\n",
        "# files.download(f'/content/saprot/upload_files/{timestamp}.csv')\n",
        "# print(fa_df[5:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5fOAAfpZGkm"
      },
      "source": [
        "## 4.3: Dataset Split\n",
        "\n",
        "Please click the run button to upload your .csv dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xtadHW9vBI7o"
      },
      "outputs": [],
      "source": [
        "#@title 4.3.1: Randomly split your .csv dataset <a name=\"split_dataset\"></a>\n",
        "\n",
        "csv_dataset_path = upload_file(UPLOAD_FILE_HOME)\n",
        "dataset_df = pd.read_csv(csv_dataset_path)\n",
        "\n",
        "split = ['train', 'valid', 'test']\n",
        "split_ratio = [0.8, 0.1, 0.1]\n",
        "\n",
        "if ('stage' not in dataset_df.columns) or (dataset_df[\"stage\"].nunique()<3):\n",
        "  dataset_df[\"stage\"] = np.random.choice(split, size=len(dataset_df), p=split_ratio).tolist()\n",
        "\n",
        "dataset_df.to_csv(csv_dataset_path, index=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCLIlgzZZJVW"
      },
      "source": [
        "## 4.4: Multiple AA Sequences & Mutation Information -> Multiple AA Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yZTseGkuBI7o"
      },
      "outputs": [],
      "source": [
        "#@title 4.4.1: Multiple AA Sequences & Mutation Information ->  Multiple AA Sequences\n",
        "csv_dataset_path = upload_file(UPLOAD_FILE_HOME)\n",
        "dataset_df = pd.read_csv(csv_dataset_path)\n",
        "\n",
        "mutation_aa_seq_list = []\n",
        "\n",
        "def seq_mut_2_seq(seq, mut):\n",
        "  if mut == ' ' or mut is None:\n",
        "    return seq\n",
        "\n",
        "  seq_list = list(seq)\n",
        "  for m in mut.split(\":\"):\n",
        "    pos = int(m[1:-1])\n",
        "    mut_aa = m[-1]\n",
        "\n",
        "    seq_list[pos-1] = mut_aa\n",
        "\n",
        "  return ''.join(seq_list)\n",
        "\n",
        "\n",
        "for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Processing...\"):\n",
        "  seq = row['Sequence']\n",
        "  mut_info = row['mutation']\n",
        "\n",
        "  mut_seq = seq_mut_2_seq(seq, mut_info)\n",
        "  mutation_aa_seq_list.append(mut_seq)\n",
        "\n",
        "mutation_aa_seq_df = pd.DataFrame(mutation_aa_seq_list, columns=['Sequence'])\n",
        "mutation_aa_seq_df.to_csv(csv_dataset_path, index=False)\n",
        "\n",
        "csv_dataset_path = upload_file(UPLOAD_FILE_HOME)\n",
        "dataset_df = pd.read_csv(csv_dataset_path)\n",
        "\n",
        "# def seq_mut_2_seq(seq, mut):\n",
        "#     if pd.isnull(mut):  # 如果突变为空，直接返回原始序列\n",
        "#         return seq\n",
        "#     else:\n",
        "#         seq_list = list(seq)\n",
        "#         for m in mut.split(\":\"):\n",
        "#             pos = int(m[1:-1])\n",
        "#             mut_aa = m[-1]\n",
        "\n",
        "#             seq_list[pos-1] = mut_aa\n",
        "\n",
        "#         return ''.join(seq_list)\n",
        "\n",
        "# for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Processing...\"):\n",
        "#     seq = row['Sequence']\n",
        "#     mut_info = row['mutation']\n",
        "\n",
        "#     mut_seq = seq_mut_2_seq(seq, mut_info)\n",
        "#     mutation_aa_seq_list.append(mut_seq)\n",
        "\n",
        "# mutation_aa_seq_df = pd.DataFrame(mutation_aa_seq_list, columns=['Sequence'])\n",
        "# mutation_aa_seq_df.to_csv(csv_dataset_path, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFuAgRlv9EiZ"
      },
      "source": [
        "# Manual <a name=\"manual\"></a>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQCdY6Pm9_FK"
      },
      "source": [
        "## How to train and share your model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbQQjnvW-WVZ"
      },
      "source": [
        "### Train your Model\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Complete the input and selection of Task Configs, and then click the run button.\n",
        "\n",
        "- `task_name` is the name of the training task you're working on.\n",
        "- `task_objective` describes the goal of your task, like sorting protein sequences into categories or predicting the values of some protein properties.\n",
        "- `base_model` is the base model you use for training. By default, it's set to the officially pretrained SaProt, but you can use models either retrained (by yourself) by ColabSaProt or shared on [SaProtHub](https://huggingface.co/SaProtHub). For example, you can choose `Trained-by-peers` with your own data if you want to retrain on Saprot models shared by others.  There are a wide range of retrained models available on [SaProtHub](https://huggingface.co/SaProtHub).\n",
        "- `data_type` indicates the kind of data you're using, which is determined by the dataset file you upload. You can find more details about the formats for different types of data in the provided <a href=\"#data_format\">instruction</a>.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-1-1.png?raw=true)\n",
        "\n",
        "<!-- <img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-1-1.png?raw=true\" height=\"400\" width=\"800px\" align=\"center\"> -->\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "After clicking the \"Run\" button, additional input boxes will appear.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-2-1.png?raw=true)\n",
        "\n",
        "Complete the input of additional information and upload files. (Note: Do not click the \"Run\" button of the next cell before completing the input and upload.)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-2-2.png?raw=true)\n",
        "\n",
        "If you want to train on an existing model, choose \"Existing Models with your data\" as `base_model` at step 1, and then \"Existing model\" input box will appear. Enter a huggingface model id or select a local model.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-2-3.png?raw=true)\n",
        "\n",
        "#### Step 3\n",
        "\n",
        "Complete the input of training configs, and then click the \"Run\" button to start training.\n",
        "\n",
        "- `batch_size` depends on the number of training samples. If your training data set is large enough, we recommend using 32, 64,128,256, ..., others can be set to 8, 4, 2. (Note that you can not use a larger batch size if you the Colab default T4 GPU. Strongly suggest you subscribe to Colab Pro for an A100 GPU.)\n",
        "- `max_epochs` refers to the maximum number of training iterations. A larger value needs more training time. The best model will be saved after each iteration. You can adjust `max_epochs` to control training duration. (Note that the max running time of Colab is 12hrs for unsubscribed user or 24hrs for Colab Pro+ user)\n",
        "- `learning_rate` affects the convergence speed of the model. Through experimentation, we have found that `5.0e-4` is a good default value for base model `SaProt_650M_AF2` and `1.0e-3` for `SaProt_35M_AF2`.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-3-1.png?raw=true)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-3-2.png?raw=true)\n",
        "\n",
        "#### Step 4\n",
        "\n",
        "You can monitor the training process by these plots. After training, check the training results and the saved model.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-4-1.png?raw=true)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/train-4-2.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "939lKY7e-iQi"
      },
      "source": [
        "### (Optional) Upload your Model to Huggingface:\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Click the \"Run\" button, the Hugging Face login interface will appear. Enter the token and click the \"Login\" button.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/upload-1-1.png?raw=true)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/upload-1-2.png?raw=true)\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "Enter the model name and model description, and then click the button to upload the model. You can check your model by the link.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/upload-2-1.png?raw=true)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/upload-2-2.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-gjYyyB-O_s"
      },
      "source": [
        "## How to use your model for prediction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUbfz_i9-pkk"
      },
      "source": [
        "### Classification&Regression prediction task\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Complete the input and selection of Task Configs, and then click the run button.\n",
        "\n",
        "- `task_objective` describes the goal of your task, like sorting protein sequences into categories or predicting the values of some protein properties.\n",
        "- `use_model_from` depends on whether you want to use a local model or a Huggingface model. If you choose `SaProtHub Models`, please enter the Hugging Face model ID in the input box. If you choose `Local Model`, simply select your local model from the options. Additionally, there's a wide range of models available on SaProtHub.\n",
        "- `data_type` indicates the kind of data you're using, which determines the dataset file you should upload. You can find more details about the formats for different types of data in the provided <a href=\"#data_format\">instruction</a>.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/cls_regr-1-1.png?raw=true)\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "After clicking the \"Run\" button, additional input boxes and upload button will appear.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/cls_regr-2-1.png?raw=true)\n",
        "\n",
        "Complete the input of additional information and upload files. (Note: Do not click the \"Run\" button of the next cell before completing the input and upload.)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/cls_regr-2-2.png?raw=true)\n",
        "\n",
        "#### Step 3\n",
        "\n",
        "Click the run button to start predicting.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/cls_regr-3-1.png?raw=true)\n",
        "\n",
        "Check your results after finishing prediction.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/cls_regr-3-2.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwc_ZYR0-s3M"
      },
      "source": [
        "### Mutational effect prediction task\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Complete the selection of Task Configs, and then click the run button.\n",
        "\n",
        "- `mutation_task` indicates the type of mutation task. You can choose from `Single-site or Multi-site mutagenesis` and `Saturation mutagenesis`.\n",
        "- `data_type` indicates the kind of data you're using, which determines the dataset file you should upload. You can find more details about the formats for different types of data in the provided <a href=\"#data_format\">instruction</a>.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-1-1.png?raw=true)\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "After clicking the \"Run\" button, additional input boxes and upload button will appear.\n",
        "\n",
        "For a single sequence, enter the sequence and the mutation information into the corresponding input fields. (Note that for Saturation mutagenesis, you won't see the Mutation input box.)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-2-1.png?raw=true)\n",
        "\n",
        "For multiple sequences, click the upload button to upload your dataset. (Note that for Saturation mutagenesis, you don’t need to provide mutation information in your dataset, which means only `sequence` column is required in the .csv dataset.)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-2-2.png?raw=true)\n",
        "\n",
        "#### Step 3\n",
        "\n",
        "Click the run button to start predicting.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-3-1.png?raw=true)\n",
        "\n",
        "Check your results after finishing prediction.\n",
        "\n",
        "For a single sequence, the predicted score will be show in the output.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-3-2.png?raw=true)\n",
        "\n",
        "For multiple sequence, the predicted score will be saved in a .csv file.\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-3-3.png?raw=true)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-3-4.png?raw=true)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "P5fOAAfpZGkm"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
